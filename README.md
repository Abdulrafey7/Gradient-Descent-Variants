# Gradient Descent Variants

This project explores **optimization techniques** by implementing:
- Gradient Descent (GD)
- Stochastic Gradient Descent (SGD)
- Mini-Batch Gradient Descent

The goal was to compare their **convergence behavior, speed, and trade-offs** during model training.

## 🚀 Features
- Hands-on implementation of GD, SGD, and Mini-Batch
- Visualization of convergence patterns
- Comparison of training efficiency

## 📊 Results
- GD → stable but slower  
- SGD → faster updates but noisy convergence  
- Mini-Batch → balance between speed & stability  

## 🛠️ Tech Stack
- Python
- NumPy
- Matplotlib

## ▶️ How to Run
```bash
git clone https://github.com/Abdulrafey7/Gradient-Descent-Variants.git
cd Gradient-Descent-Variants
jupyter notebook
