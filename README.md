# Gradient Descent Variants

This project explores **optimization techniques** by implementing:
- Gradient Descent (GD)
- Stochastic Gradient Descent (SGD)
- Mini-Batch Gradient Descent

The goal was to compare their **convergence behavior, speed, and trade-offs** during model training.

## ğŸš€ Features
- Hands-on implementation of GD, SGD, and Mini-Batch
- Visualization of convergence patterns
- Comparison of training efficiency

## ğŸ“Š Results
- GD â†’ stable but slower  
- SGD â†’ faster updates but noisy convergence  
- Mini-Batch â†’ balance between speed & stability  

## ğŸ› ï¸ Tech Stack
- Python
- NumPy
- Matplotlib

## â–¶ï¸ How to Run
```bash
git clone https://github.com/Abdulrafey7/Gradient-Descent-Variants.git
cd Gradient-Descent-Variants
jupyter notebook
